import math

import pandas as pd

from bpllib import utils

METRIC = 'accuracy'
DATASETS = utils.easy_datasets + ['-'] + utils.medium_datasets + ['-'] + utils.hard_datasets
METHODS = ['Find-RS', 'RIPPER', 'ID3', 'AQ', 'SVM', 'RF']
df = pd.read_pickle('../results.pkl')

dbase = df[(df['strategy'] == 'single')]
dbp = df[(df['strategy'] == 'bp')]

print("% autogenerated by baselines_table.py")
print("""
\\begin{table}[ht]
\\centering
\\small

\\begin{tabular}{ l | l l l l l l l l}
\\hline
""")

for method in METHODS:
    print(f'& {method} ', end='')
print('\\\\ \\hline')

for dataset in DATASETS:
    if dataset == '-':
        print(f'\\hline')
        continue
    if dataset == 'LYMPHOGRAPHY':
        print(f'\\texttt{{LYMPH.}} ', end='')
    else:
        print(f'\\texttt{{{dataset}}} ', end='')

    best_method = None
    best_metric = float('-inf')

    for method in METHODS:
        avgs = []
        avgs_base = []
        stds = []
        for enc in ["av", "ohe"]:
            avg_metric = dbp[(dbp['dataset'] == dataset) & (dbp['method'] == method) & (dbp['encoding'] == enc)][
                METRIC].mean()
            avg_base_metric = \
                dbase[(dbase['dataset'] == dataset) & (dbase['method'] == method) & (dbase['encoding'] == enc)][
                    METRIC].mean()
            std_metric = dbp[(dbp['dataset'] == dataset) & (dbp['method'] == method) & (dbp['encoding'] == enc)][
                METRIC].std()
            avgs.append(avg_metric)
            avgs_base.append(avg_base_metric)
            stds.append(std_metric)

        best_enc_idx = avgs.index(max(avgs))
        best_enc = ["av", "ohe"][best_enc_idx]
        avg_metric = avgs[best_enc_idx]
        avg_base_metric = avgs_base[best_enc_idx]
        std_metric = stds[best_enc_idx]

        if avg_metric > best_metric:
            best_method = method
            best_metric = avg_metric

    for method in METHODS:
        avgs = []
        stds = []
        for enc in ["av", "ohe"]:
            avg_metric = dbp[(dbp['dataset'] == dataset) & (dbp['method'] == method) & (dbp['encoding'] == enc)][
                METRIC].mean()
            std_metric = dbp[(dbp['dataset'] == dataset) & (dbp['method'] == method) & (dbp['encoding'] == enc)][
                METRIC].std()
            avgs.append(avg_metric)
            stds.append(std_metric)

        best_enc_idx = avgs.index(max(avgs))
        best_enc = ["av", "ohe"][best_enc_idx]
        avg_metric = avgs[best_enc_idx]
        std_metric = stds[best_enc_idx]
        marker = "" if best_enc == "av" else "$\\dagger$"

        if math.isnan(avg_metric):
            # put a dash
            print(f'& - ', end='')
        elif math.isnan(std_metric):
            # avoid +- nan for 1 seed
            if method == best_method:
                print(f'& \\textbf{{{avg_metric:.4f}}} {marker} ', end='')
            else:
                print(f'& {avg_metric:.4f} {marker} ', end='')
        else:
            if method == best_method:
                print(f'& \\textbf{{{avg_metric:.4f}}} \\tiny{{$\\pm$ {std_metric:.3f}}} {marker} ', end='')
            else:
                print(f'& {avg_metric:.4f}  \\tiny{{$\\pm$ {std_metric:.3f}}} {marker} ', end='')
    print('\\\\')

# Add average ranks

print(f'\\hline')
print(f'\\textbf{{AvgRank}} ', end='')

method_avgs = []
for method in METHODS:
    avgs = []
    stds = []
    for dataset in DATASETS:
        if dataset == '-':
            continue
        av_avg_metric = df[(df['dataset'] == dataset) & (df['method'] == method) & (df['encoding'] == 'av')][
            METRIC].mean()
        oh_avg_metric = df[(df['dataset'] == dataset) & (df['method'] == method) & (df['encoding'] == 'oh')][
            METRIC].mean()
        avg_metric = max(av_avg_metric, oh_avg_metric)
        avgs.append(avg_metric)
    method_avgs.append(avgs)

sc = pd.DataFrame(method_avgs)

avg_ranks = (sc.T).rank(axis=1, ascending=False, method='average').mean(axis=0)

for method, avg_rank in zip(METHODS, avg_ranks):
    print(f'& {avg_rank:.2f} ', end='')
print('\\\\')

print("""
\\end{tabular}
\\caption{\\label{tab:baselines} Comparison of the baseline performance using """ + METRIC + """. 
One-hot encoding was used if it was the best-performing one (marked with $\dagger$). Results are averaged across 10 runs.}

\\end{table}
""")
